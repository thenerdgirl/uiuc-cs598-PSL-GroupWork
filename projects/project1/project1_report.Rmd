---
title: "Project 1: Predict the Housing Prices in Ames"
subtitle: "CS598: Practical Statistical Learning"
author: "Naomi Bhagat - nbhagat3, Michael Miller - msmille3, Joe May - jemay3"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
urlcolor: cyan
---

# Assignment Data

Program: MCS-DS
Assignment post: [campuswire](https://campuswire.com/c/G06C55090/feed/193)


Team contributions:

| Person         | Contribution                                |
|----------------|---------------------------------------------|
| Naomi Bhagat   | Data pre-procesing, traning/testing general process, report Section 1 |
| Michael Miller |                                             |
| Joe May        |                                             |

# Section 1: Technical Details

Discuss details such as data pre-processing and other non-trivial implementation aspects of your models. Do NOT paste your code in the report. Instead, explain the technical steps in clear English. Your description should be comprehensive enough for your fellow PSL classmates to replicate your results.

For instance, when documenting your pre-processing steps, provide specifics such as:
- Which variables did you exclude from the analysis?
- Identify the variables treated as categorical. How were these variables encoded, were any levels merged, etc?
- For numerical variables, were there any transformations applied?
- You’re not required to justify these pre-processing decisions; just state what was done.
When documenting implementation, general statements like “We use lasso to fit a sparse regression model” are insufficient. Instead, aim for detailed descriptions, such as: “We utilized lasso for regression modeling. Specifically, we employed the glmnet function in R with the data standardized and with lambda set to lambda.min.”

--> For this assignment, we created two models: a linear-based modeland a tree-based model. The following sections go into deeper details about our design choices.

## Pre-Processing

The first step we took for this project was to clean up the data with some data pre-processing steps. The pre-processing 
steps described in this section are common between the linear and tree models that were built.

There were several pre-processing steps necessary for the linear model. First, we replaced NULL or missing values in the
"Garage Year Built" column, as it was the only column in the data with missing values. Next, we removed a set of 
imbalanced categorical variables, and variables that don't offer additional insights. Most of these variables are highly 
biased, meaning that the entries for these vairables skew heavily in favor of one categry rather than a more normal 
distribution. The following is the set of removed variables:

- Street
- Utilities
- Condition_2
- Roof_Matl
- Heating
- Pool_QC
- Misc_Feature
- Low_Qual_Fin_SF
- Pool_Area
- Longitude
- Latitude

The next pre-processing step was Winsorization. Because the impact of some of the area-related variables need a ceiling,
we calculated the 95% upper quantile of the chosen variables based on the training data, and any value in both the 
training and test datasets that exceeds this value is replaced with that value, effectively capping the possibe values 
in the feature at this 95% quantile value. The winsorized variables include:

- Lot_Frontage
- Lot_Area
- Mas_Vnr_Area
- BsmtFin_SF_2
- Bsmt_Unf_SF
- Total_Bsmt_SF
- Second_Flr_SF
- First_Flr_SF
- Gr_Liv_Area
- Garage_Area
- Wood_Deck_SF
- Open_Porch_SF
- Enclosed_Porch
- Three_season_porch
- Screen_Porch
- Misc_Val

Next, we converted our remaining categorical variables into 1-hot vectors. TODO @Michael or @Joe describe this more in detail please!! Also explain the final step about adding Sale Price back in.

## Linear Model

After running the training data through the described pre-processing, we created our linear model. First, we trained an 
initial model using glmnet. The sole purpose of this model is to get the most optimal set of variables to use for 
prediction purposes. Then, using only this set of selected variables from the intial model, we created a second glmnet 
model which is then used to make the final prediction on the test data.

@Michael, @Joe - maybe add some challenges here? Talk about the forcing of columns to be the same between train and test?

## Tree Model

For our tree model, we initially tried using a randomForest, bt no matter how muh we tweaked the model, we were not 
getting good results on the 10 splits. Therefore, we switched to using an xgboost model, largely using trial and error 
to determine which model was the most optimal for our purposes.

# Section 2: Performance Metrics

Report the accuracy of your models on the test data (refer to the provided evaluation metric below), the execution time 
of your code, and details of the computer system you used (e.g., Macbook Pro, 2.53 GHz, 4GB memory or AWS t2.large) for 
all 10 training/test splits.

@Joe, @Michael - this section is all yours :)
