---
title: "Project 3 - Vocab Generation"
subtitle: "CS598: Practical Statistical Learning"
author: 
 - name: Naomi Bhagat - nbhagat3
 - name: Michael Miller - msmill3
 - name: Joe May - jemay3
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: readable
    toc: yes
urlcolor: cyan
---

First, install some packages to help us out with the vocab generation. We can also set the seed here for model creation.

```{r step1, eval = TRUE}
# packages to load
packages = c('text2vec', 'glmnet')

# if packages don't exist, install. Then call library on them
for (package in packages) {
  if (!requireNamespace(package, quietly=TRUE)) {
    install.packages(package)
  }
  library(package, character.only=TRUE)
}

# set seed
set.seed(235)
```

Next, we initialize some variables, specifically the stop words for vacb generation and the vocabulary size. We choose the same stop words as the professor provides on Campuswire (https://campuswire.com/c/G06C55090/feed/626), and select a vocabulary size that is small enough to be interpretable.

```{r step2, eval = TRUE}
# set the stop words
stop_words = c("i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", "yours", "their", "they", 
               "his", "her", "she", "he", "a", "an", "and", "is", "was", "are", "were", "him", "himself", "has", "have",
               "it", "its", "the", "us")

# top points vocab size
vocab_size = 1000
```

Now, gather all the training data. In order to avoid repeating the same steps over and over again for each split, we append all the training data together into one table, and use this as the collective training data. We had the option to use test data, but did not choose this route, because in a real-wprld scenario, there is no test data to access in the training stage.

```{r step3, eval = TRUE}
train = read.table("split_1/train.tsv", stringsAsFactors = FALSE, header = TRUE)

for (split_num in 2:5) {
  split_dir =  paste0('split_', as.character(split_num))
  temp_train = read.table(paste0(split_dir, '/train.tsv'), stringsAsFactors = FALSE, header = TRUE)
  train = rbind(train, temp_train)
}
```

This next chunk is taken entirely from the professor's Campuswire post (https://campuswire.com/c/G06C55090/feed/626). Here, we clean the HTML tags from the training data, then use the text2vec package to construct the document term matrix, maximum 4-grams. 

```{r step4, eval = TRUE}
# clear out the html tags in the review
train$review = gsub('<.*?>', ' ', train$review)

# create an iterator over tokens
it_train = itoken(train$review, preprocessor = tolower, tokenizer = word_tokenizer)

# build the vocabulary
tmp.vocab = create_vocabulary(it_train, stopwords = stop_words, ngram = c(1L,4L))

# prune the vocabulary
tmp.vocab = prune_vocabulary(tmp.vocab, term_count_min = 10, doc_proportion_max = 0.5, doc_proportion_min = 0.001)

# construct a document-term matrix
dtm_train = create_dtm(it_train, vocab_vectorizer(tmp.vocab))
```

Next, we use Lasso regression with Logistic Regression to train a model, using the document term matrix as the x value, the training sentiment values as y, and the binomial family type.

Note here that the team had some confusion about how to trainthis model before understanding that Logistic Regression was required; to help explain the difference between this and Cross-Validation, we used the following StackOverflow post: https://stackoverflow.com/questions/29311323/difference-between-glmnet-and-cv-glmnet-in-r.

```{r step5, eval = TRUE}
model = glmnet(x = dtm_train, y = train$sentiment, alpha = 1, family="binomial")
```

Now, we have to go through the coefficients of the model to determine which vocabulary terms from the document matrix to use. First, we select all the columns with a non-zero beta, and then we can choose the beta that is the highest while staying under the pre-determined vocabulary size (in this case, 1000). This chosen column is then used to create the final vocabulary. 

For this step in particular, the team received hints from a former student for how best to trim down the vocabulary size. 

```{r step6, eval = TRUE}
nonzero_beta = colSums(model$beta != 0)
best_col_index = unname(which.max(which(nonzero_beta < vocab_size)))
vocabulary = colnames(dtm_train)[which(model$beta[, best_col_index] != 0)]
```

Finally, we write the vocabulary to the output file.

```{r step7, eval = TRUE}
write.table(vocabulary, file = "myvocab.txt", row.names = FALSE, col.names = FALSE, sep = "", quote = FALSE)
```
