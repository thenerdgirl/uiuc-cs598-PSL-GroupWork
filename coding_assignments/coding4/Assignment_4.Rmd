---
title: "Coding Assignment 4"
subtitle: "CS598: Practical Statistical Learning"
author: 
 - name: Naomi Bhagat - nbhagat3
 - name: Michael Miller - msmille3
 - name: Joe May - jemay3
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: readable
    toc: yes
urlcolor: cyan
---

# Assignment Data
Program: MCS-DS  
Team contributions:

| Person         | Contribution |
|----------      |----------|
| Naomi Bhagat   | Part 1, Part 2 |
| Michael Miller |  |
| Joe May        |  |

Assignment URL: [campuswire post](https://campuswire.com/c/G06C55090/feed/432)

```{r setup, eval = TRUE}
# add required packages
packages = c()

# if packages don't exist, install. Then call library on them
for (package in packages) {
  if (!requireNamespace(package, quietly=TRUE)) {
    install.packages(package)
  }
  library(package, character.only=TRUE)
}
```


# Part 1: Gaussian Mixtures

Implement the EM algorithm from scratch for a p-dimensional Gaussian mixture model with G components:

## Part 1-1: Estep function
This function should return an n-by-G matrix, where the (i, j)th entry represents the conditional probability P(Zi = k | xi). Here i ranges from 1 to n and k ranges from 1 to G.

```{r part1-1, eval = TRUE}
Estep = function(data, G, init_params) {
  # Use Approach 2
  A = t(data) # transpose of the data matrix
  temp = matrix(0, nrow=nrow(data), ncol=G)
  for(i in 1:G) {
    mu = init_params$mean[,k]
  }
}
```

## Part 1-2: Mstep function
This function should return the updated parameters for the Gaussian mixture model.

```{r part1-2, eval = TRUE}
Mstep = function(data, G, init_params, post_prob) {

}
```

## Part 1-3: loglik function
This function computes the log-likelihood of the data given the parameters.
```{r part1-3, eval = TRUE}
loglik = function(data, G, init_params) {
  A = t(data)
  P = ncol(data)
  temp = matrix(0, nrow)
}
```

## Part 1-4: myEM function (main function): Inside this function, you can call the Estep, Mstep, and loglik functions.
### Input
- data: The dataset.
- G : The number of components.
- Initial parameters.
- itmax: The number of iterations.
### Output:
- prob: A G-dimensional probability vector (p1,…,pG)
- mean: A p-by-G matrix with the k-th column being μk, the p-dimensional mean for the k-th Gaussian component.
- Sigma: A p-by-p covariance matrix Σ shared by all G components;
- loglik: A number equal to ∑ni=1log[∑Gk=1pk⋅N(x;μk,Σ)].
```{r part1-4, eval = TRUE}
myEM = function(data, G, init_params, itmax) {
  for(i in 1:itmax) {
    post_prob = Estep(data, G, init_params)
    init_params = Mstep(data, G, init_params, post_prob)
  }

  init_params$loglik = loglik(data, G, init_params)
  return(init_params)
}
```



## Part1-5: Testing
```{r part1-5, eval = TRUE}

# Case 1: G = 2. Using variables from assignment
G = 2
p1 = 10/n
p2 = 1 - p1
u1 = 5 #@todo fix me
u2 = 7 #@todo fix me

max_iteration = 20




# Case 2: G = 3
G = 3
p1 = 10/n
p2 = 1 - p1
u1 = 5 #@todo fix me
u2 = 7 #@todo fix me

max_iteration = 20


```



# Part 2: HMM

Implement the Baum-Welch (i.e., EM) algorithm and the Viterbi algorithm from scratch for a Hidden Markov Model (HMM) that produces an outcome sequence of discrete random variables with three distinct values.

## Part 2-1: Baum-Welch Algorihtm

The Baum-Welch Algorihtm is the EM algorithm for the HMM. Create a function named BW.onestep designed to carry out the E-step and M-step. This function should then be called iteratively within myBW.

```{r part2-1, eval = TRUE}
# forward/backward algorithms adapted from https://www.adeveloperdiary.com/data-science/machine-learning/forward-and-backward-algorithm-in-hidden-markov-model/

forward_prob = function(x, params) {
  T = length(x)
  A = params$A
  B = params$B
  w = params$w
  mz = params$mz
  
  # Calculate first row
  alpha = matrix(0, T, mz)
  alpha[1,] = w*B[, x[1]]
  
  # Recursive part
  for(t in 2:T) {
    temp = alpha[t-1,] %*% A
    alpha[t,] = temp*B[, x[t]]
  }
  
  return(alpha)
}

backward_prob = function(x, params) {
  T = length(x)
  A = params$A
  B = params$B
  w = params$w
  mz = params$mz
  
  # Initialize beta as a matrix of ones, since the last row is all ones
  beta = matrix(1, T, mz)
  
  # Recursive part, but backwards
  for(t in (T-1):1) {
    temp = as.matrix(beta[t+1,]*B[, x[t+1]])
    beta[t,] = t(A %*% temp)
  }
  
  return(beta)
}

BW.onestep = function(x, params) {
  T = length(x)
  A = params$A
  B = params$B
  w = params$w
  mx = params$mx
  mz = params$mz
  
  # Compute forward, backward probabilities
  alpha = forward_prob(x, params)
  beta = backward_prob(x, params)
  
  # Compute gamma (E-step)
  gamma = array(0, dim=c(mz, mz, T-1))
  for(t in 1:(T-1)) {
    for(i in 1:mz) {
      for(j in 1:mz) {
        gamma[i, j, t] = alpha[t, i] * A[i, j] * B[j, x[t+1]] * beta[t+1, j]
      }
    }
  }

  # Update param A (M-step)
  A = rowSums(gamma, dims = mz)
  A = A/rowSums(A)
  
  # Update param B (M-step again)
  temp = apply(gamma, c(1,3), sum)
  temp = cbind(temp, colSums(gamma[, , T-1]))
  for(i in 1:mx) {
    B[, i] = rowSums(temp[, which(x==i)])
  }
  B = B/rowSums(B)
  
  params$A = A
  params$B = B
  return(params)
}

myBW = function(x, params, itmax) {
  for(i in 1:itmax) {
    params = BW.onestep(x, params)
  }
  return(params)
}
```

## Part 2-2: Viterbi Algorithm

This algorithm outputs the most likely latent sequence considering the data and the MLE of the parameters.

`myViterbi:`
Input:
- data: a T-by-1 sequence of observations
- parameters: mx, mz, w, A and B
Output:
- Z: A T-by-1 sequence where each entry is a number ranging from 1 to mz.

```{r part2-2, eval = TRUE}
myViterbi = function(data, params) {
  T = length(data)
  A = params$A
  B = params$B
  w = params$w
  mx = params$mx
  mz = params$mz
  
  # Compute delta in log scale
  delta = matrix(0, T, mz)
  delta[1,] = log(w) + log(B)[, data[1]]
  
  # Recursive formula, super similar to alpha calculation from BW
  for(t in 2:T) {
    for(i in 1:mz) {
      delta[t, i] = max(delta[t-1,] + log(A[, i])) + log(B[i, data[t]]) 
    }
  }
  
  # Z calculation using delta
  # Similar to the backwards calculation of beta
  Z = rep(0, T)
  Z[T] = which.max(delta[T,])
  for(t in (T-1):1) {
    Z[t] = which.max(delta[t,] + log(A[, Z[t+1]]))
  }
  
  return(Z)
}

```

## Part 2-3: Testing

```{r part2-3, eval = TRUE}
data = scan("coding4_part2_data.txt")

mz = 2
mx = 3

A = matrix(1, mz, mz)
B = matrix(1:6, mz, mx)
w = c(0.5, 0.5)

# Initialize A, B
A = A / rowSums(A)
B = B / rowSums(B)

params = list(A = A,
              B = B,
              w = w,
              mx = mx,
              mz = mz
              )

# Get output
output = myBW(data, params, 100)
Z = myViterbi(data, output)

output$A
output$B
Z
```
