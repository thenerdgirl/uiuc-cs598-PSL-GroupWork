---
title: "Coding Assignment 3"
subtitle: "CS598: Practical Statistical Learning"
author: 
 - name: Naomi Bhagat - nbhagat3
 - name: Michael Miller - msmille3
 - name: Joe May - jemay3
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: readable
    toc: yes
urlcolor: cyan
---

# Assignment Data
Program: MCS-DS  
Team contributions:

| Person         | Contribution |
|----------      |----------|
| Naomi Bhagat   | Part 1 |
| Michael Miller | Part 2 |
| Joe May        |  |

Assignment URL: [campuswire post](https://campuswire.com/c/G06C55090/feed/242)

```{r setup, eval = TRUE}
# add required packages
packages = c('splines')

# if packages don't exist, install. Then call library on them
for (package in packages) {
  if (!requireNamespace(package, quietly=TRUE)) {
    install.packages(package)
  }
  library(package, character.only=TRUE)
}

# set seed
set.seed(235)

```


# Part 1: Optimal Span for LOESS
## Part 1-1: Computing the Diagonal of the Smoother Matrix

Create a function to retrieve the diagonal of the smoother matrix. Weâ€™re only
interested in the diagonal entries (which will be used in computing LOO-CV and
GCV), so this function should return an n-by-1 vector.
- Inputs: x (an n-by-1 feature vector) and span (a numerical value).
- Output: n-by-1 vector representing the diagonal of the smoother matrix S.
- Tip: Review the technique we used for the smoother matrix in smoothing spline 
models and adapt it for LOESS.

```{r part1-1, eval = TRUE}
get_diag = function(x, span) {
  # x: n-by-1 feature vector
  # span: numerical value for span
  
  n = length(x)
  
  # return vector d (diagonal)
  d = rep(0, n)
  
  # Smoothing matrix S
  S = diag(n)
  
  # Algorithm
  for(i in 1:n) {
    fit = loess(S[ , i] ~ x, span=span)
    y_hat = fit$fitted
    d[i] = y_hat[i]
  }
  
  return(d)
}
```

### Part 1-2
Span Value Iteration:
- Iterate over the specified span values.
- For each span, calculate the CV and GCV values.
- Post iteration, compile lists of CV and GCV values corresponding to each span.

```{r part1-2, eval = TRUE}
span_value_iter = function(x, y, span) {
  n = length(x)
  num_span = length(span)
  
  # return value vectors
  cv = rep(0, num_span)
  gcv = rep(0, num_span)
  
  # Iterate over the span values
  for(s in 1:num_span) {
    # For each span, calculate cv and gcv values
    # First, fit the loess model
    fit = loess(y ~ x, span=span[s], control=loess.control(surface="direct"))
    residuals = fit$residuals
    d = get_diag(x, span[s])
    
    # intermediate calculations for cv and gcv
    # https://liangfgithub.github.io/Notes/lec_W5_NonlinearRegression.pdf slide 33
    tr = sum(d)
    sse = sum(residuals ^ 2)
    
    # calculate cv
    cv[s] = sum((residuals / (1 - d))^2) / n
    
    # calculate gcv
    gcv[s] = sse / (n * (1 - (tr / n)) ^ 2)
  }
  
  return(list(cv = cv, gcv = gcv))
}
```

### Part 1-3
1. Test your code using data set [Coding3_Data.csv]
2. Report your CV and GCV for the following 15 span values: 0.20, 0.25, . . . , 0.90.
3. Report the optimal span value(s) based on CV and GCV.
4. Display the original data points and overlay them with the true curve and the fitted curve(s) generated using the optimal span value(s).

The true curve is \[ f(x) = \frac{\sin(12(x + 0.2))}{x + 0.2} \]

```{r part1-3, eval = TRUE}
part1_data = read.csv("Coding3_Data.csv")
test_span = c(0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9)
result = span_value_iter(part1_data$x, part1_data$y, test_span)

# Report cv, gcv values
output = data.frame(CV = result$cv, GCV = result$gcv, span = test_span)
output

# Optimal cv values
cv_opt = output$span[output$CV == min(output$CV)]
cv_opt

# Optimal gcv values
gcv_opt = output$span[output$GCV == min(output$GCV)]
gcv_opt

# Plot curve
# Data + Control curve
plot(part1_data$x, part1_data$y, xlab="x", ylab="y")

fx = 1:50/50
fy = sin(12*(fx + 0.2)) / (fx + 0.2)
lines(fx, fy, col="red")

# My curve - cv
cv_f = loess(y ~ x, part1_data, span=cv_opt)
lines(fx, predict(cv_f, data.frame(x = fx)), lty=2, col="green")

# My curve - gcv
gcv_f = loess(y ~ x, part1_data, span=gcv_opt)
lines(fx, predict(gcv_f, data.frame(x = fx)), lty=2, col="blue")
```

# Part 2: Clustering Time Series
## Preparation

```{r part2-0, eval = TRUE}
sales_df = read.csv('Sales_Transactions_Dataset_Weekly.csv')

# Per https://campuswire.com/c/G06C55090/feed/346, only keep W columns
sales_df = subset(sales_df, select = grepl('W', names(sales_df)))

#de-mean
sales_df = sales_df - sapply(sales_df, function(x) colMeans(sales_df))

X = sales_df
```

## Part 2-1: Fitting NCS
- Fit each time series with an NCS with df = 10. This corresponds to an NCS with 8 interior knots. Each row of X represents the response, with the 1-dimensional feature being the index from 1 to 52.
- Store the NCS coefficients (excluding the intercept) in an 811-by-9 matrix B

```{r part2-1, eval = TRUE}
# spline for each var
spline_count = ncol(X)
weeks = 1:52
df = 10

# get a natural cubic spline for each column, store the coefs (except intercept)
B = matrix(0, nrow=spline_count, ncol=df-1)
for(i in 1:spline_count) {
  B[i, ] = smooth.spline(x=weeks, y=X[i,], df=10)$y[1:df-1]
}

```


Part 2-2: Clustering using Matrix B
```{r part2-1, eval = TRUE}
#6 clusters per instructions 
k = 6

kmeans_result = kmeans(B[1, ], centers = K)

```


Part 2-3: Clustering using Matrix X 




