---
title: "Coding Assignment 1"
subtitle: "CS598: Practical Statistical Learning"
author: 
 - name: Naomi Bhagat - nbhagat3
 - name: Michael Miller - msmill3
 - name: Joe May - jemay3
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: readable
    toc: yes
urlcolor: cyan
---

# Work Breakdown

| Person         | Contribution |
|----------      |----------|
| Naomi Bhagat   | asdf |
| Michael Miller | fgjg |
| Joe May        | fgjj |

# Part 1: Generate Data
## Part 1-1
First generate the 20 centers from two-dimensional normal. You can use any mean and covariance structure.  You should not regenerate the centers. Use these 20 centers throughout this simulation study.
```{r part1-1, eval = TRUE}
# Set Seed
set.seed(235)

# import necessary libraries
library(class)

csize = 10       
p = 2        
s = sqrt(1/5)                         
m1 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(1,csize), rep(0,csize) )
m0 = matrix(rnorm(csize*p), csize, p)*s + cbind( rep(0,csize), rep(1,csize) )
```

## Part 1-2
Given the 20 centers, generate a training sample of size 200 (100 from each class) and a test sample of size 10,000 (5,000 from each class).
```{r part1-2, eval = TRUE}
# Training sample of size 200
n = 100
id1 = sample(1:csize, n, replace=TRUE)
id0 = sample(1:csize, n, replace=TRUE)  
traindata = matrix(rnorm(2*n*p), 2*n, p)*s + rbind(m1[id1,], m0[id0,])
dim(traindata)
Ytrain = rep(c(1, 0), each = n)

# Test sample of size 10,000

N = 5000
# Allocate the n samples for class 1  to the 10 clusters
id1 = sample(1:csize, N, replace=TRUE)
# Allocate the n samples for class 1 to the 10 clusters
id0 = sample(1:csize, N, replace=TRUE)  
testdata = matrix(rnorm(2*N*p), 2*N, p)*s + rbind(m1[id1,], m0[id0,])
dim(testdata)
Ytest = rep(c(1, 0), each = N)
```

## Part 1-3
Produce a scatter plot of the training data:  
  - assign different colors to the two classes of data points;  
  - overlay the 20 centers on this scatter plot, using a distinguishing marker (e.g., a star or a different shape) and color them according to their respective class.
```{r part1-3, eval = TRUE}
plot(traindata[,1], traindata[,2], type="n", xlab="", ylab="")
points(traindata[1:n,1], traindata[1:n,2], col="green")
points(traindata[(n+1):(2*n),1], traindata[(n+1):(2*n),2], col="red")
points(m1[1:csize,1], m1[1:csize,2], pch="+", cex=1.5, col="green")
points(m0[1:csize,1], m0[1:csize,2], pch="+", cex=1.5, col="red");   
legend("bottomright", pch = c(1,1), col = c("red", "green"), legend = c("class 1", "class 0"))
```

# Part 2: kNN


Implement kNN from scratch; use Euclidean Distance. Your implementation should meet the following requirements:

    Input: Your kNN function should accept three input parameters: training data, test data, and k. No need to write your kNN function to handle any general input; it suffices to write a function that is able to handle the data for this specific simulation study: binary classification; features are two-dimensional numerical vectors.
    Output: Your function should return a vector of predictions for the test data.
    Vectorization: Efficiently compute distances between all test points and training points simultaneously. Make predictions for all test points in a single operation.
    No Loops: Do not use explicit loops like for or while inside your kNN function to compute distances or make predictions. Instead, harness the power of vectorized operations for efficient computations. For example, you can use broadcasting in Numpy or command outer in R.



```{r part2, eval = FALSE}
# Euclidean helper function
euclidean_distance = function(point1, point2) {
  if (length(point1) == length(point2)) {
    return(sqrt(sum((point1 - point2)^2)))
  } else {
    stop('Vectors must be of the same length')
  }
}


knn_from_scratch = function(train, test, truth, k) {
  neighbor_distances = outer(1:nrow(test), 1:nrow(train), Vectorize(function(i, j) euclidean_distance(test[i,], train[j,])))
  # Calculate each point's distance to neighbors
  
  nearest_neighbor_address = function(vector, j){
    order(vector)[1:j]
  }
  # Function to get k nearest neighbors (temp variable j for k)
  
  nearest_neighbor_address = t(apply(neighbor_distances, 1, nearest_neighbor_address, j = k))
  # Apply function to get k nearest neighbors for each point in a vectorized manner
  # nearest_neighbor_address
  
  get_vote = function(indices, vote){
    vote[indices]
  }
  # Function to collect votes for each point's k nearest neighbors
  
  neighborhood_votes = t(apply(nearest_neighbor_address, 1, get_vote, vote = truth))
  # Apply function for each point to collect votes for each point's 
  # k nearest neighbors in a vectorized manner
  # neighborhood_votes
  
  determine_winner = function(point){
    if (sum(point) > k/2) {
      return(1)
    } else if (sum(point) < k/2) {
      return(0)
    } else {
      return(sample(0:1, 1))
    }
  }
  # Determine winner given votes of k nearest neighbors. Ties are broken randomly.
  
  predictions = apply(neighborhood_votes, 1, determine_winner)
  return(predictions)
}

# 2. Explain how you handle distance ties and voting ties; distance ties may
#   occur when you have multiple (training) observations that are equidistant
#   from a test observation. voting ties may occur when K is an even number and
#   you have 50% of the k-nearest-neighbors from each of the two classes.
  
# Ans: If the number of votes for each point is the same, we have decided to 
#   randomly generate a class for that particular test point because the two
#   classes at that point are equally as likely.

#  3. Test your code with the training/test data you just generated when 
#   K = 1, 3, 5; and compare your results with knn in R or sklearn.neighbors in
#   Python. Report your results (on the test data) as a 2-by-2 table
#   (confusion matrix) for each K value. Report the results from knn or
#   sklearn.neighbors as a 2-by-2 table (confusion matrix) for each K value

# k = 1
baseline_test.pred = knn(traindata, testdata, Ytrain, k=1)
table(Ytest, baseline_test.pred)
implementation_test.pred = knn_from_scratch(traindata, testdata, Ytrain, k=1)
table(Ytest, implementation_test.pred)

# k = 3
baseline_test.pred = knn(traindata, testdata, Ytrain, k=3)
table(Ytest, baseline_test.pred)
implementation_test.pred = knn_from_scratch(traindata, testdata, Ytrain, k=3)
table(Ytest, implementation_test.pred)

# k = 5
baseline_test.pred = knn(traindata, testdata, Ytrain, k=5)
table(Ytest, baseline_test.pred)
implementation_test.pred = knn_from_scratch(traindata, testdata, Ytrain, k=5)
table(Ytest, implementation_test.pred)
```

# Part 3: cvkNN
## Part 3-1
Implement KNN classification with K chosen by 10-fold cross-validation from scratch.  
- Set the candidate K values from 1 to 180. (The maximum candidate K value is 180. Why?)  
- From now on, you are allowed to use the built-in kNN function from R or Python instead of your own implementation from Part 2.  
- It is possible that multiple K values give the (same) smallest CV error; when this happens, pick the largest K value among them, since the larger the K value, the simpler the model.

We choose 180 as the max K value because there will be 180 samples in our training dataset 

```{r part3-1, eval = TRUE}

# init variables for looping through k values
max_k = 180
fold_count = 10
k_values = 1:max_k
average_errors = rep(0, max_k)
fold_size = floor(nrow(traindata) / fold_count)
random_idx = sample(1:nrow(traindata))

# loop through values of k, for each, calculate n-fold cv error
for (k in k_values){
  # cumulative error for averaging out at the end
    error = 0
    
  for (fold in 1:fold_count){
    # calculate the index for train/validate for this fold 
    # do this by getting ordered idx (e.g. 1:10), then using that to get rand idx
    ordered_val_idx = ((fold - 1) * fold_size + 1) : (fold * fold_size)
    val_idx = random_idx[ordered_val_idx]
    
    # split data into train/val. Use negative index to exclude ones we don't want
    fold_train_x = traindata[-val_idx, ]
    fold_train_y = Ytrain[-val_idx]
    fold_val_x = traindata[val_idx, ]
    fold_val_y = Ytrain[val_idx]
    
    # Make prediction
    fold_predicted_y = knn(fold_train_x, fold_val_x, fold_train_y, k=k)
      
    # Sum the error
    error = error + sum(fold_predicted_y != fold_val_y)
  }
    
  # Add error to array to track
  average_errors[k] = error / fold_count
}

# Find lowest error (larger k breaks tie) which.max returns first, so reversing
# array will get us last
plot(average_errors, xlab="k", ylab="Average error count", main="Error vs K")
min_idx = length(average_errors) - which.min(rev(average_errors)) + 1
min_error = average_errors[min_idx]
best_k = k_values[min_idx]
```

## Part 3-2
Test your code with the training/test data you just generated. Report your results (on the test data) as a 2-by-2 table and also report the value of the selected K.
```{r part3-2, eval = TRUE}
# now train model using our preferred k
predicted = knn(traindata, testdata, Ytrain, k=best_k)
actual = Ytest
print(table(actual, predicted))
```

The optimal value of k is `r best_k` and it results in an average validation error of `r  min_error`.
